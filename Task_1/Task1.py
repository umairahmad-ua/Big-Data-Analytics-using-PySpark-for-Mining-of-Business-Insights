# -*- coding: utf-8 -*-
"""BDA Assignment 2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ENjMAVqylqb1cLGxqVK4OJNWt9VkRx8l

# **Assignment 2 Big Data Analytics**
Group Member 1: Umair Ahmad     21-2081

Group Member 2: Khadija Mehmood 21i-2313

# **Learning Outcomes of This Assignment:**

---
Learn to model the entire problem by analyzing and inspecting
* Learn to use docker image
* Learn to install and use Apache Spark in docker image and manual
* Learn to use beautiful soup
* Learn to use Scrapping of webiste
* Learn to understand and apply Data Analystics on five different business problems
* Understand the Command line for parallel programming
* Understand the modeling of Spark
* Understand the MLlib
* Understand the Data Modeling 
* Understand database with scheme
* Learn to cleanse, normalize, shard, spark for query mapping
* Lean to use Data frame of Spark
* Learn to work on substantial real-world big data ;
* Understand the data cleansing processes by writing PYTHON scripts
* Understand how Visualize the data to translate information into a visual context
(Matplotlib)
* Use an appropriate tool to query and drive insights from data e.g.
Relational Db with Dataframes.
* Improve reporting of data analysis

# **2. Problems Encounters:**

---


* Really difficult to implement Firebase integration with python
* Difficult to handle rough scrapped data
* First attempt on Docker spark
* Unseen Commands
* Understating of MLib working
* Spark SQL Data frame working
* Unavailability of core libraires of python in sprak docker image
* Handling of a lot of empty values
* Plotting issues with continuous data


# **3. Solution to Problems Encounters:**


---


* Install the docker and import spark image
* Read the articles and implement some pilot examples
* Read the articles and implement some pilot examples
* Read the articles and implement some pilot examples
* Solve basic word count and average problem first and the move to the real problem
* Use the basic python and develop all the logic by myself
* Replace the Nonempty values with Nan
* Delete the Nan values for plotting
* Apply pivot, histogram, and binning logic to solve this problem

#Introduction


---

In **Big Data Analytics** Assignment, we are assigned to perform multiple tasks. We want to build a system to help property investors, buyers, sellers, and zameen.com
company. Our system will help to analyze and decide whether he should buy or sell a
property in a particular location.

DATASET : Zameen.com.

#Installing and importing libraries
"""

!pip install firebase
!pip install python_jwt 
!pip install gcloud
!pip install sseclient
!pip install Crypto
!pip install pycryptodome
! pip install requests_toolbelt

"""#Task # 01 : Scraping

---
We are assigned to scrape data from any link from bunch of given links of zameen.com website. Our data must be more than 300 rows. 
we extracted more than 10K rows for better analysis.


"""

#importing relevent libraries.
import requests
import re
import math
from bs4 import BeautifulSoup

"""##Generating Links for Scraping.



we used 4 links and then navigated upto 100 webpages from each link and extracted more links for features extraction and made a list of almost 400 links...So this is pretty enough amount of links to gather data.  **:)**  
"""

lst=[]
for x in range (1,100):
  link1="https://www.zameen.com/Flats_Apartments/Islamabad-3-1.html"
  link2="https://www.zameen.com/Houses_Property/Islamabad-3-1.html"
  link3="https://www.zameen.com/Rentals_Houses_Property/Islamabad-3-1.html"
  link4="https://www.zameen.com/Rentals_Flats_Apartments/Islamabad-3-1.html"
  link1 = link1[:-6] + str(x) + link1[-6+1:]
  lst.append(link1)
  link2 = link2[:-6] + str(x) + link2[-6+1:]
  lst.append(link2)
  link3 = link3[:-6] + str(x) + link3[-6+1:]
  lst.append(link3)
  link4 = link4[:-6] + str(x) + link4[-6+1:]
  lst.append(link4)
len(lst)

"""we had choice to use whether *Selenium* or *Beautiful soup*.

We Used Beautiful Soup for scraping.
"""

pages=[]
for x in range(len(lst)):
  page = requests.get(lst[x]) #generting request for 400 links.
  soup = BeautifulSoup(page.content, 'html.parser')
  pages.append(soup)

final_links=[]
for i in range (len(pages)):
  lst2=pages[i].find_all("a", class_="_7ac32433")
  for x in range(len(lst2)):
    ad_link="https://www.zameen.com"+str(lst2[x]["href"])
    final_links.append(ad_link)
len(final_links)# we get details for 10102 properties.

import pandas as pd
dict1 = {'links': final_links}
df = pd.DataFrame(dict1)
df.to_csv('links.csv') #saving links into CSV

"""##Scraping and Generting Data.

Features mentioned below that we required for insights.

property id
price
location
city
property type
area
bedrooms
baths
property name
purpose.
"""

pro_names, pro_types,pro_prices,pro_purposes,pro_locations,pro_baths,pro_beds=[],[],[],[],[],[],[] #Declaring features variables as list.
pro_cities,pro_area_names,pro_ids,pro_descriptions,pro_areas=[],[],[],[],[]
for x in final_links:
  page = requests.get(x)
  soup = BeautifulSoup(page.content, 'html.parser') 
  print(x)

  if(soup.find(class_="_64bb5b3b")!=None):
    pro_name=soup.find(class_="_64bb5b3b").getText()
    pro_type=soup.find(class_="_812aa185").getText()
    pro_price = soup.find_all('span', attrs={'aria-label':'Price'})
    pro_area = soup.find_all('span', attrs={'aria-label':'Area'})
    pro_type = soup.find_all('span', attrs={'aria-label':'Type'})
    pro_purpose = soup.find_all('span', attrs={'aria-label':'Purpose'})
    pro_location = soup.find_all('span', attrs={'aria-label':'Location'})
    pro_bath = soup.find_all('span', attrs={'aria-label':'Baths'})
    pro_bed = soup.find_all('span', attrs={'aria-label':'Beds'})

    #Standardization of price and area
    pro_price=pro_price[-1].getText()
    if('Lakh' in pro_price):
      pro_price=pro_price.replace('Lakh','')
      pro_price=pro_price.replace('PKR','')
      pro_price=str(math.ceil(float(pro_price))*100000)
    elif('Crore' in pro_price):
      pro_price=pro_price.replace('Crore','')
      pro_price=pro_price.replace('PKR','')
      pro_price=str(math.ceil(float(pro_price))*10000000)
    elif('Thousand' in pro_price):
      pro_price=pro_price.replace('Thousand','')
      pro_price=pro_price.replace('PKR','')
      pro_price=str(math.ceil(float(pro_price))*1000)
    pro_type=pro_type[-1].getText()
    pro_purpose=pro_purpose[-1].getText()
    pro_location=pro_location[-1].getText()
    pro_bath=pro_bath[-1].getText()
    pro_bed=pro_bed[-1].getText()
    pro_description=soup.find(class_="_2a806e1e").getText()
    lst=pro_location.split(',')
    pro_area_name=lst[0]
    pro_area=pro_area[-1].getText()
    if('Kanal' in pro_area):
      pro_area=pro_area.replace('Kanal','')
      pro_area=str(float(pro_area)*20)
    elif('Marla' in pro_area):
      pro_area=pro_area.replace('Marla','')
    pro_city=lst[-2]
    pro_city=pro_city.replace(' ','')
    pro_id = soup.find_all('span', attrs={'aria-label':'Link name'})
    pro_id=pro_id[-1].getText()
    pro_names.append(pro_name)
    pro_types.append(pro_type)
    pro_price=re.sub("[^0-9]", "", pro_price)
    print(pro_price)
    pro_prices.append(int(pro_price))
    pro_purposes.append(pro_purpose)
    pro_locations.append(pro_location)
    pro_baths.append(pro_bath)
    pro_beds.append(pro_bed)
    pro_cities.append(pro_city)
    pro_area_names.append(pro_area_name)
    pro_ids.append(pro_id)
    pro_descriptions.append(pro_description)
    pro_areas.append(pro_area)


#saving list in to dictionaries,
dict1 = {'Property_ID': pro_ids,'Property_Name': pro_names,'Property_Type': pro_types,'Property_Purpose': pro_purposes'Baths': pro_baths,'Bedrooms': pro_beds,'City': pro_city,
      '}
dict2 = {'Property_ID': pro_ids,'Property_Location': pro_locations,'Area':pro_areas,'City': pro_city,
      'Area_Name': pro_area_names,'Properity_Price': pro_prices}

df = pd.DataFrame(dict1) # converting dictionary into Dataframe
df.to_csv('data.csv') #creating CSV from dataframe
