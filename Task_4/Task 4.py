# -*- coding: utf-8 -*-
"""BDA Assignment 2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ENjMAVqylqb1cLGxqVK4OJNWt9VkRx8l

# **Assignment 2 Big Data Analytics**
Group Member 1: Umair Ahmad     21-2081

Group Member 2: Khadija Mehmood 21i-2313

# **Learning Outcomes of This Assignment:**

---
Learn to model the entire problem by analyzing and inspecting
* Learn to use docker image
* Learn to install and use Apache Spark in docker image and manual
* Learn to use beautiful soup
* Learn to use Scrapping of webiste
* Learn to understand and apply Data Analystics on five different business problems
* Understand the Command line for parallel programming
* Understand the modeling of Spark
* Understand the MLlib
* Understand the Data Modeling 
* Understand database with scheme
* Learn to cleanse, normalize, shard, spark for query mapping
* Lean to use Data frame of Spark
* Learn to work on substantial real-world big data ;
* Understand the data cleansing processes by writing PYTHON scripts
* Understand how Visualize the data to translate information into a visual context
(Matplotlib)
* Use an appropriate tool to query and drive insights from data e.g.
Relational Db with Dataframes.
* Improve reporting of data analysis

# **2. Problems Encounters:**

---


* Really difficult to implement Firebase integration with python
* Difficult to handle rough scrapped data
* First attempt on Docker spark
* Unseen Commands
* Understating of MLib working
* Spark SQL Data frame working
* Unavailability of core libraires of python in sprak docker image
* Handling of a lot of empty values
* Plotting issues with continuous data


# **3. Solution to Problems Encounters:**


---


* Install the docker and import spark image
* Read the articles and implement some pilot examples
* Read the articles and implement some pilot examples
* Read the articles and implement some pilot examples
* Solve basic word count and average problem first and the move to the real problem
* Use the basic python and develop all the logic by myself
* Replace the Nonempty values with Nan
* Delete the Nan values for plotting
* Apply pivot, histogram, and binning logic to solve this problem

#Introduction


---

In **Big Data Analytics** Assignment, we are assigned to perform multiple tasks. We want to build a system to help property investors, buyers, sellers, and zameen.com
company. Our system will help to analyze and decide whether he should buy or sell a
property in a particular location.

DATASET : Zameen.com.

#Installing and importing libraries
"""

!pip install firebase
!pip install python_jwt 
!pip install gcloud
!pip install sseclient
!pip install Crypto
!pip install pycryptodome
! pip install requests_toolbelt



"""#Task # 02 : RDBMS

we used Firebase Realtime Database.

# ERD

from firebase import firebase
FBconn=firebase.FirebaseApplication("https://my-project-48021-default-rtdb.firebaseio.com/",None)

"""##loading data in to Database"""

FBconn.post('/MyTestData',dict1) # table 1
FBconn.post('/MyTestData',dict2) # table 2
te=FBconn.get('/MyTestData/',"-N1E8ANSTPTAcmcJVRYD")
print(te)

"""# Task # 3 : Data Analysis with Apache Spark."""

def rename_col(data):
  return data.rename(columns = {'Properity ID':'Property_ID', 'Properity Name':'Property_Name', 'Properity Type':'Property_Type','Properity Purpose':'Property_Purpose','Properity Price':'Property_Price',
                     'Properity Location':'Property_Location','Area Name':'Area_Name','Beds':'Bedrooms'}, inplace = True)

import pandas as pd
data2 = pd.read_csv("data (2).csv")
rename_col(data2)
data2.to_csv("data (2).csv", index=False)

import csv
import math
data = pd.read_excel("dataNew.xlsx")
data = data.iloc[: , 1:]
rename_col(data)
data['Area_Name'] = data['Area_Name'].replace({'Bahria Town': 'Bahria Town Lahore'})

data.head()

def round_conversion(data):
    try:
        if "Sq. Yd." in data:
            return math.ceil(int(data.split(' ')[0])*0.0330578)
    except:
        return math.ceil(data)



data["Area"] = data["Area"].apply(lambda x: round_conversion(x))

data.to_csv("dataNew.csv", index=False)

"""# Insight 1 Price Prediction"""

!pip install pyspark

from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate() 
spark

rdd=spark.read.option('header','true').csv("data.csv",inferSchema=True)

rdd.show()

rdd.schema

rdd=rdd.na.fill(0)
rdd=rdd.na.fill("Unknown")

rdd.columns

from pyspark.ml import Pipeline
from pyspark.ml.feature import StringIndexer

indexers = [StringIndexer(inputCol=column, outputCol=column+'_index', handleInvalid='keep').fit(rdd) for column in ["Area","Baths","Bedrooms","Property_Purpose","Property_Type","Area_Name"]]


pipeline = Pipeline(stages=indexers)
data = pipeline.fit(rdd).transform(rdd)
data.show()

cols = ("Property_ID","Property_Name","Property_Purpose","Property_Type","Property_Location","Area","Baths","Bedrooms","City","Area_Name")
data=data.drop(*cols)
data.schema

from pyspark.ml.feature import VectorAssembler
vectorAssembler = VectorAssembler(inputCols = data.drop("Property_Price").columns, outputCol = 'features').setHandleInvalid("keep")

data = vectorAssembler.transform(data)

data.toPandas().to_csv('mycsv.csv')

train, test= data.randomSplit([0.8,0.2],seed=7)

train.describe().show()

# Simple baseline (linreg)

from pyspark.ml.regression import LinearRegression

lr = LinearRegression(featuresCol = 'features', labelCol='Property_Price', maxIter=10, 
                      regParam=0.8, elasticNetParam=0.1) # It is always a good idea to play with hyperparameters.
lr_model = lr.fit(train)

trainingSummary = lr_model.summary
print("RMSE: %f" % trainingSummary.rootMeanSquaredError)
print("r2: %f" % trainingSummary.r2)

lr_predictions = lr_model.transform(test)
lr_predictions.select("prediction","Property_Price","features").show(30)

from pyspark.ml.evaluation import RegressionEvaluator
lr_evaluator = RegressionEvaluator(predictionCol="prediction", \
                 labelCol="Property_Price",metricName="r2")
print("R Squared (R2) on val data = %g" % lr_evaluator.evaluate(lr_predictions))

import matplotlib.pyplot as plt
df=data.toPandas()
X=df["Area_index"]
y=df["Property_Price"]
plt.style.use('default')
plt.style.use('ggplot')

fig, ax = plt.subplots(figsize=(8, 4))
df2=lr_predictions.toPandas()
ax.scatter(df2["Area_index"], df2["prediction"], color='red', label='Regression model')
ax.scatter(X, y, edgecolor='k', facecolor='grey', alpha=0.7, label='Sample data')
ax.set_ylabel('Price', fontsize=14)
ax.set_xlabel('Area', fontsize=14)
ax.legend(facecolor='white', fontsize=11)
fig.tight_layout()

fig, ax = plt.subplots(figsize=(8, 4))
df2=lr_predictions.toPandas()

ax.scatter(df2["Bedrooms_index"], df2["prediction"], color='red', label='Regression model')
ax.scatter(df["Bedrooms_index"], df["Property_Price"], edgecolor='k', facecolor='grey', alpha=0.7, label='Sample data')
ax.set_ylabel('Price', fontsize=14)
ax.set_xlabel('Bedrooms', fontsize=14)
ax.legend(facecolor='white', fontsize=11)
fig.tight_layout()

fig, ax = plt.subplots(figsize=(8, 4))
df2=lr_predictions.toPandas()

ax.scatter(df2["Baths_index"], df2["prediction"], color='red', label='Regression model')
ax.scatter(df["Baths_index"], df["Property_Price"], edgecolor='k', facecolor='grey', alpha=0.7, label='Sample data')
ax.set_ylabel('Price', fontsize=14)
ax.set_xlabel('Baths', fontsize=14)
ax.legend(facecolor='white', fontsize=11)
fig.tight_layout()

"""# 2nd Insight which location is most popular"""

In2_RDD=spark.read.option('header','true').csv("data (2).csv",inferSchema=True)

In2_RDD.show()

In2_RDD.createOrReplaceTempView("Housing_Data")
In2_RDD2 = spark.sql("SELECT Property_Location AS Property_Location , Property_Purpose AS Property_Purpose  from Housing_Data")
In2_RDD2.printSchema()
In2_RDD2.show()
type(In2_RDD2)

Location = In2_RDD2.select("Property_Location")
Location_Rent = In2_RDD2.filter(In2_RDD2["Property_Purpose"]=="For Rent")
Location_Sale = In2_RDD2.filter(In2_RDD2["Property_Purpose"]=="For Sale")

Location.show()
Location_Rent.show()
Location_Sale.show()

Location=Location.toPandas()
Location_Rent=Location_Rent.toPandas()
Location_Sale=Location_Sale.toPandas()

n = 10
print(Location.value_counts()[:n].index.tolist())
print(Location_Rent.value_counts()[:n].index.tolist())
print(Location_Sale.value_counts()[:n].index.tolist())

"""## 3rd Insight. which property type is most demanding."""

In3_RDD=spark.read.option('header','true').csv("data (2) .csv",inferSchema=True)

In3_RDD.show()

In3_RDD.createOrReplaceTempView("Housing_Data")
In3_RDD2 = spark.sql("SELECT Property_Type AS Property_Type from Housing_Data")
In3_RDD2.printSchema()
In3_RDD2.show()

Pro_type=In3_RDD2.toPandas()

print(Pro_type.value_counts()[:1].index.tolist())

"""# 4th Insight comparison of colony"""

In4_RDD=spark.read.option('header','true').csv("dataNew.csv",inferSchema=True)

In4_RDD.show()

In4_RDD.createOrReplaceTempView("Housing_Data")
In4_RDD2 = spark.sql("SELECT Area AS Area , Area_Name As Area_Name , Property_Price As Property_Price from Housing_Data")
In4_RDD2.printSchema()
In4_RDD2.show()

Bahria_Rawalpindi = In4_RDD2.filter(In4_RDD2["Area_Name"]=="Bahria Town Rawalpindi")
Bahria_Lahore = In4_RDD2.filter(In4_RDD2["Area_Name"]=="Bahria Town Lahore")
Bahria_Karachi = In4_RDD2.filter(In4_RDD2["Area_Name"]=="Bahria Town Karachi")

Bahria_Rawalpindi.show()
Bahria_Lahore.show()
Bahria_Karachi.show()

from pyspark.sql.functions import col, avg, corr , when

Avg_Rawalpindi =  Bahria_Rawalpindi.select(avg('Property_Price')).collect()
Avg_Lahore =  Bahria_Lahore.select(avg('Property_Price')).collect()
Avg_Karachi = Bahria_Karachi.select(avg('Property_Price')).collect()

print("Avg_rate of property price in Rawalpindi", Avg_Rawalpindi ,"\n" ,"Avg_rate of property price in Lahore", Avg_Lahore ,"\n" , "Avg_rate of property price in Karahi",Avg_Karachi )
print("Most expensive place is Bahria Rawalpindi ", max(Avg_Rawalpindi,Avg_Lahore,Avg_Karachi))

"""5 marla rate in each bahria colony"""

Rwp_Bahria_5 = Bahria_Rawalpindi.where(col('Area') == 5).select(col('Property_Price')).collect()
LHR_Bahria_5 = Bahria_Lahore.where(col('Area') == 5).select(col('Property_Price')).collect()
Kar_Bahria_5 = Bahria_Karachi.where(col('Area') == 5).select(col('Property_Price')).collect()
print("Rate of 5 marla area in Rawalpindi", Rwp_Bahria_5[0] ,"\n" ,"Rate of 5 marla area in Lahore", LHR_Bahria_5[0] ,"\n" , "Rate of 5 marla area in Karahi", Kar_Bahria_5[0] )

Rwp_Bahria_5 = Bahria_Rawalpindi.select(Bahria_Rawalpindi[''])

"""# 5th Insight which property is most expensive."""

priceindex = In4_RDD.select("Property_Price").toPandas().idxmax()
priceindex

print(In4_RDD.select(['Property_ID','Property_Name','Property_Location',
                        'Property_Type','Area','Area_Name','Property_Price',
                        'Property_Purpose']).collect()[int(priceindex)])

"""#correlation of area with price"""

print(" Correlation between Area and property price " , In4_RDD2.stat.corr("Area", "Property_Price"))

"""# Visualization

# Z-score

Z-score is a parametric measure and it takes two parameters â€” mean and standard deviation.
Once you calculate these two parameters, finding the Z-score of a data point is easy.

I used an arbitrary threshold of 2, beyond which all data points are flagged as outliers.
The rule of thumb is to use 2, 2.5, 3 or 3.5 as threshold.
"""

from scipy import stats
refined_data = []

property_types=data2["Property_Type"].unique().tolist()

for property_type in property_types:
    property_type_df=data2[data2['Property_Type'].str.contains(property_type)]
    z = np.abs(stats.zscore(property_type_df['Property_Price']))
    property_type_df= property_type_df[(np.abs(stats.zscore(property_type_df['Property_Price'])) < 2)]
    refined_data.append(property_type_df)
refined_data = pd.concat(refined_data)

ax = sns.boxplot(x="Property_Type", y="Property_Price", data=refined_data)

df[df['city'].str.contains('Lahore')].groupby(["bed"] )['bed'].count().plot(kind="bar",figsize=(20, 5),logy=True,
                         title="Cities and the amount of Data in them")

"""## Data Normalaization
 Normalizing the right skewed SalePrice

"""

fig, ax =plt.subplots(1,2)
sns.distplot(refined_data['price'], ax=ax[0])
sns.distplot(np.sqrt(refined_data['price']), ax=ax[1])
plt.xlabel('log10(price)')
fig.show()

refined_data["log_price"]=np.round(np.sqrt(refined_data['price']), 3)

plt.scatter(refined_data['log_area_sqfeet'], refined_data['log_price'])
plt.xlabel("Area")
plt.ylabel("Price")
plt.show()

refined_data.drop(columns=['price','area_sqfeet'],inplace=True)

harea_index_list = refined_data[refined_data['log_area_sqfeet'] > 7].index.tolist()
larea_index_list = refined_data[refined_data['log_area_sqfeet'] < 2].index.tolist()
price_index_list = refined_data[refined_data['log_price'] < 4].index.tolist()



refined_data.drop(harea_index_list, inplace=True)
refined_data.drop(larea_index_list, inplace=True)
refined_data.drop(price_index_list, inplace=True)

# Again plottng the LotArea - SalePrice graph after the outlier removal

plt.scatter(refined_data["log_area_sqfeet"], refined_data["log_price"])
plt.title("After Outliers Removal")
plt.xlabel("Lot Area")
plt.ylabel("log10(Sale Price)")
plt.show()

"""## Remove Outliers from each property type"""

extreme_outliers_data = []
for property_type in property_types:
    property_type_df=data2[data2['Property_Type'].str.contains(property_type)]
    z = np.abs(stats.zscore(property_type_df['Property_Price']))
    property_type_df= property_type_df[(np.abs(stats.zscore(property_type_df['Property_Price'])) > 2)]
    extreme_outliers_data.append(property_type_df)

flat=extreme_outliers_data[1]
house=extreme_outliers_data[0]

ax = sns.boxplot(x="Property_Type", y="Property_Price", data=data2)

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import os
import seaborn as sns
import matplotlib as plt
from math import sin, cos, sqrt, atan2, radians
import plotly.express as px
from scipy import stats
import ast


sns.set(rc={'figure.figsize':(11.7,8.27)})

data2.head()

city_comparison=df[df['area'].str.contains('1 Kanal1 Kanal|10 Marla|3 Marla3 Marla') & df['city'].str.contains("Lahore|Karachi|Islamabad|Rawalpindi")].groupby(["city","area"])['price'].median().to_frame('price').reset_index()
city_comparison.style.background_gradient(cmap='summer')

sns.lineplot(data=city_comparison, x="area", y="price", hue="city")

"""## Insight 2 visualizations

Top 10 Locations in Pakistan
"""

top_10=data2.groupby(["Property_Location"])['Property_Location'].count().nlargest(10).to_frame('count').reset_index()
top_10.index=top_10["Property_Location"]
top_10.drop(columns=['Property_Location'],inplace=True)
top_10.plot.pie(y='count', figsize=(15, 8))

"""Top 10 Locations for Rent"""

top_10=Location_Rent.groupby(["Property_Location"])['Property_Location'].count().nlargest(10).to_frame('count').reset_index()
top_10.index=top_10["Property_Location"]
top_10.drop(columns=['Property_Location'],inplace=True)
top_10.plot.pie(y='count', figsize=(15, 8))

"""Top 10 Locations for sale"""

top_10=Location_Sale.groupby(["Property_Location"])['Property_Location'].count().nlargest(10).to_frame('count').reset_index()
top_10.index=top_10["Property_Location"]
top_10.drop(columns=['Property_Location'],inplace=True)
top_10.plot.pie(y='count', figsize=(15, 8))

"""## Insight 4 Visualization

Which city's Bahria is more expensive
"""

data.groupby(["Area_Name"])['Area_Name'].count().plot(kind="bar",figsize=(20, 5),logy=True,
                         title="Area_Name and the amount of Data in them")

data3 = pd.read_csv("dataNew.csv")
data3.head()

"""##correlation Heat map"""

heatmap = sns.heatmap(data2.corr(), vmin=-1, vmax=1, annot=True)
# Give a title to the heatmap. Pad defines the distance of the title from the top of the heatmap.
heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);